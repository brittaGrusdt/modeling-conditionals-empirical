var PARAMS_TO_FIT = data["par_fit"]

var observed_counts = data["observed_utts_ratios"]
var contexts = _.uniq(_.map(observed_counts, 'id'))

var par_zoibs = data["likelihoods_zoib"]
var par_gaussian = data["likelihoods_gaussian"]
var prior_rel = data["prior_relations"]

var prior_samples_with_weights = data["p_s_ci"]

var speaker_type = data["speaker_type"]
//display(prior_samples_with_weights)
/*
// ************ States used for model predictions for each trial ************ //
// get Bayes nets used for predictions for each trial + participant
// we sample a certain number of Bayes nets per relation
// (depending on P(r|data), computed beforehand in R)
// // ONE DISTRIBUTION FOR EACH CN: PRIOR CONDITIONED ON CN
*/
var priors_conditioned = map(function(r) {
  var prior_conditioned_r = Infer({model: function() {
      var s = sample(globalStore.state_prior)
      condition(s.r == r)
      return(s)
    }, method: 'enumerate'})
  return([r, prior_conditioned_r])
}, RELATIONS)
var prior_conditioned_r = Object.fromEntries(priors_conditioned)

// var posterior_states_contexts = get_posterior_given_context(
//   prior_conditioned_r, par_zoibs, par_gaussian, contexts, prior_rel
// )
// load precomputed weights
var posterior_states_contexts = build_posterior_states_given_context_from_data(
  prior_samples_with_weights, contexts
)
//display(posterior_states_contexts.if1_uh.support())

var non_normalized_posterior = function() {
  var params = priorSample(PARAMS_TO_FIT)
  setParams(params)
  //display(params)
  // with current sampled set of parameters, are there any states where no
  // utterance is applicable or any utterance without state?
  // if yes do not consider these parameters
  var invalid_utts_states = check_states_utts(
    ALL_BNS,
    _.map(globalStore.utterances, 'utt'),
    globalStore.thresholds,
    params,
    false
  )
  condition(invalid_utts_states.states.length == 0)
  condition(invalid_utts_states.utts.length == 0)

  // RUN MODEL
  var rsa_speaker_predictions = run_speaker(ALL_BNS, speaker_type, false)

  // predictions 4 (close to) certain worlds
  var predictions_certain_worlds_list = map(function(bn){
    return([bn.w, rsa_speaker_predictions[bn.bn_id]])
  }, BNS_CERTAIN_WORLD)
  var predictions_certain_worlds = Object.fromEntries(predictions_certain_worlds_list)
  var speaker_predictions = predictions_with_gamma(rsa_speaker_predictions,
                                                   predictions_certain_worlds)

  // get predictions + compute log likelihood of utterance choices given model predictions
  var log_likelihoods = map(function(context) {
    var observed_ci = filter(function(obj){
      return(obj.id == context)
    }, observed_counts)

    var Weights = posterior_states_contexts[context]
    ////////////compute model prediction based on repeated set of draws from weights /////////////
    // var n_rep = 100
    // var results_ci = reduce(function(i_rep, acc){
    //   var n_ci = sum(_.map(observed_ci, 'n'))
    //   var states = repeat(n_ci, function(){return(sample(Weights))})
    //   var states_ids = _.map(states, 'bn_id')
    //   var counts = _.countBy(states_ids) // but a state can be sampled more than once
    //   // summarize bn_ids so that each bn_id occurs once and probabilities summed up accoordingly
    //   // necessary for get_weighted_rsa_prediction - function!
    //   var states_to_probs = Object.fromEntries(
    //     map(function(id_occ){
    //       var p = (1 / states_ids.length) * id_occ[1]
    //       var id = id_occ[0]
    //       return([id, p])
    //     }, Object.entries(counts))
    //   )
    //   var model_prediction = get_weighted_rsa_prediction(
    //     speaker_predictions, Object.keys(states_to_probs), Object.values(states_to_probs)
    //   )
    //   var result = Object.fromEntries(zip(model_prediction.params.vs,
    //                                       model_prediction.params.ps))
    //   return(acc.concat(result))
    // }, [], _.range(0, n_rep))
    // //display(results_ci)
    //
    // var avg_predictions = Object.fromEntries(map(function(u){
    //     var predictions_u = _.map(results_ci, u)
    //     var avg_p = sum(predictions_u) / predictions_u.length
    //     return([u, avg_p])
    //   }, _.map(globalStore.utterances, 'utt'))
    // )
    // //display(avg_predictions)
    // var model_prediction = Categorical({
    //   vs: Object.keys(avg_predictions),
    //   ps: Object.values(avg_predictions)
    // })
    // display(model_prediction)
   ////////////////////////////////
    var pars_weights = Object.values(Weights.params)
    var p_states = pars_weights[1]
    var states_ids = _.map(pars_weights[0], 'bn_id')
    var model_prediction = get_weighted_rsa_prediction(
      speaker_predictions, states_ids, p_states
    )
    var ll = get_log_likelihood_by_context(observed_ci, model_prediction)
    return(ll)
  }, contexts)

  var summed_log_likelihood = sum(log_likelihoods)
  //display(summed_log_likelihood)

  factor(summed_log_likelihood)

  //query.add("utts", params.utt_cost)
  //query.add("alpha", params.alpha)
  query.add("theta", params.theta)

  if(PARAMS_TO_FIT.includes("gamma")){
    query.add("gamma", params.gamma)
  }
  if(speaker_type != "literal"){
    query.add("alpha", params.alpha)
  }

  return(query)

  // I dont know why but if I return the object 'params' directly, I only
  // get the latest value back ... so for 100 final samples, I get 100 values,
  // but always the same even though when printing the sampled parameter values,
  // these are different (as they should)
  // return({
  //   alpha: params.alpha,
  //   theta: params.theta,
  //   gamma: params.gamma,
  //   utt_cost: params.utt_cost
  // })
}
