// ************ Other model parameters ************ //
globalStore.utterances = reduce(function(utt, acc) {
    var obj = {utt: utt, type: getUtteranceType(utt)}
    return(acc.concat(obj))
  }, [], data["utterances"])

// ************ Default free model parameters ************ //
var PAR_DEFAULT = {alpha: 6.36, gamma: 0.368, theta: 0.9, theta_might: 0,
  cost_conditional: 0, cost_might: 0, cost_conjunction: 0, cost_literal: 0,
  // default: all utterances equally likely
  p_utts: T.div(ones([globalStore.utterances.length, 1]), globalStore.utterances.length)
}

// ************ Observed data to compute likelihoods ************ //
// IMPORTANT for posterior-by-trial: OBSERVATIONS must contain all utterances +
// trial combinations, even though n is 0, i.e. utterance was not observed
var OBSERVATIONS = data["observations"]
var TRIAL_IDS = _.uniq(_.map(OBSERVATIONS, 'id'))
var PROLIFIC_IDS = _.uniq(_.map(OBSERVATIONS, 'prolific_id'))

// {trial_id: {prolific_id: observed_utt}}
var observations_by_trial_list = map(function(trial_id){
  var observed_trial = filter(function(obj){
    return(obj.id == trial_id)
  }, OBSERVATIONS)
  var subjects = _.map(observed_trial, 'prolific_id')
  var obj = Object.fromEntries(zip(subjects, observed_trial))
  return([trial_id, obj])
}, TRIAL_IDS)
var OBSERVATIONS_BY_TRIAL_SUBJ = Object.fromEntries(observations_by_trial_list)
//display(OBSERVATIONS_BY_TRIAL_SUBJ["if1_uh"])

// ************ Prior  ************ //
// for generating tables
globalStore.n_prior_samples = data["n_forward_samples"][0]
//display(data["prior_samples"])
globalStore.state_prior = build_state_prior_from_data(data["prior_samples"])
var ALL_BNS = globalStore.state_prior.support()

// get bayes nets closest to certain world where P(x,y) is maximal, i.e. ~ 1
var BNS_CERTAIN_WORLD = map(function(world) {
  var ps_world = map(function(bn) {
    return( Math.exp(bn.table.score(world)))
  }, ALL_BNS)
  var max = reduce(function(p, acc){
    return p > acc ? p : acc
  }, 0, ps_world)
  var idx = ps_world.indexOf(max)
  var bn_world = ALL_BNS[idx]
  return({w: world, bn: bn_world, bn_id: bn_world.bn_id})
}, WORLDS)


// states applicable to utterance types
var get_states_applicable_to_utt_type = function(utt_type, thresholds) {
  var utts = filter(function(u){
    u.type == utt_type
  }, globalStore.utterances)
  var states_utt_type = filter(function(bn){
    any(function(u){meaning(u.utt, bn["table"], thresholds)}, utts)
  }, ALL_BNS)
  return(_.map(states_utt_type, 'bn_id'))
}

// ************ Functions ************ //
/** return log likelihood of observed data for 'trial_id' given model's
predictions for 'trial_id' and 'subj'
@arg model_prediction: categorical distribution over utterances
@arg trial_id: str
@arg subj: str, id from a participant
**/
var get_log_likelihood_by_trial_and_subj = function(model_prediction, trial_id, subj) {
  var utt_subj_trial = filter(function(elem){
    return(elem.id == trial_id && elem.prolific_id == subj)
  }, OBSERVATIONS)[0]['utterance']
  var log_likelihood = model_prediction.score(utt_subj_trial)
  return(log_likelihood)
}

/** return log likelihood of observed data given model's predictions for respective context
@arg model_prediction: categorical distribution over utterances
@arg observations_ci: object id:<str>, utterance:<str>, n:<int>, ratio:<double>
**/
var get_log_likelihood_by_context = function(observations_ci, model_prediction) {

  //display(_.uniq(_.map(observations_ci, 'id'))[0])
  var utts = _.map(observations_ci, 'utterance')
  var obs_counts = _.map(observations_ci, 'n')
  //var obs_ratios = _.map(observations_ci, 'ratio')
  //display("observed utt: " + utts[2] + " n: " + obs_counts[2] + " ratio: " + obs_ratios[2])

  var predicted_ratios = map(function(u) {
    var p = Math.exp(model_prediction.score(u))
    return(p)
  }, utts)
  //display(sum(predicted_ratios))
  var likelihood = Multinomial({n: sum(obs_counts), ps: predicted_ratios})

  return(likelihood.score(obs_counts))
}

//get model predictions for 'bns' using all concrete utterances or utt types
var run_speaker = function(bns, by_utt_type){
  var distrs = map(function(bn){
    var sp = speaker_utt_type(bn, false)
    var prediction = by_utt_type ? marginalize(sp, 'type') : marginalize(sp, 'utt')
    return([bn.bn_id, prediction])
  }, bns)
  //var distributions = {"speaker_": distrs, "bns": bns}
  var distributions = Object.fromEntries(distrs)
  return(distributions)
}

// draw set of parameters to be fitted
var priorSample = function(par){
  var gamma = par.includes("gamma") ? {gamma: beta({a: 1, b: 1})} : {};
  // var alpha = par.includes("alpha") ? {alpha: uniform({a: 0, b: 10})} : {};
  //var alpha = par.includes("alpha") ? {alpha: sample(lognormal(1, 1))} : {};
  var alpha = par.includes("alpha") ? {alpha: sample(Exponential({a: 0.1}), {driftKernel: exponentialKernel})} : {};
  var theta = par.includes("theta") ? {theta: beta({a: 10, b: 2})} : {};
  var theta_might = par.includes("theta_might") ? {theta_might: beta({a: 2, b: 2})} : {};
  // if cost, then fix them (just n-1 cost wrt a reference category)
  // var cost_conditional = par.includes("cost_conditional") ? {cost_conditional: uniform({a: -0.5, b: 0.5})} : {};
  // var cost_conjunction = par.includes("cost_conjunction") ? {cost_conjunction: uniform({a: -0.5, b: 0.5})} : {};
  // var cost_literal = par.includes("cost_literal") ? {cost_literal: uniform({a: -0.5, b: 0.5})} : {};
  // var cost_might = par.includes("cost_might") ? {cost_might: uniform({a: -0.5, b: 0.5})} : {};
  var ps_utts = par.includes("p_utts") ?
    {ps_utts: T.toScalars(dirichlet({alpha: ones([globalStore.utterances.length, 1])}))} : {};

  // format to retrieve more easily from R
  var utts = par.includes("p_utts") ? {utts: _.map(globalStore.utterances, 'utt')} : {};

  var all_pars = Object.assign(gamma, alpha, theta, theta_might, ps_utts, utts);
    // cost_conditional, cost_conjunction, cost_literal, cost_might);

  return all_pars
}

var setParams = function(draw){
  globalStore.alpha = draw.alpha ? draw.alpha : PAR_DEFAULT.alpha
  globalStore.gamma = draw.gamma ? draw.gamma : PAR_DEFAULT.gamma
  globalStore.thresholds = {
    theta: draw.theta ? draw.theta : PAR_DEFAULT.theta,
    theta_might: draw.theta_might ? draw.theta_might : PAR_DEFAULT.theta_might
  }
  globalStore.ps_utts = draw.p_utts ? draw.p_utts : PAR_DEFAULT.p_utts;
  // globalStore.cost = {
  //   conditional: draw.cost_conditional ? draw.cost_conditional : PAR_DEFAULT.cost_conditional,
  //   conjunction: draw.cost_conjunction ? draw.cost_conjunction : PAR_DEFAULT.cost_conjunction,
  //   literal: draw.cost_literal ? draw.cost_literal : PAR_DEFAULT.cost_literal,
  //   might: draw.cost_might ? draw.cost_might : PAR_DEFAULT.cost_might
  // }

  // informativeness of utterance types for this set of parameters
  globalStore.states_literal = get_states_applicable_to_utt_type("literal", globalStore.thresholds)
  globalStore.states_conjunction = get_states_applicable_to_utt_type("conjunction", globalStore.thresholds)
  globalStore.states_conditional = get_states_applicable_to_utt_type("conditional", globalStore.thresholds)
  globalStore.states_might = get_states_applicable_to_utt_type("might", globalStore.thresholds)
}

/** returns an object from utterances to summands of (1-gamma) part which are
computed by sum w in worlds W: P(w|s) * P_speaker(u| w)
**/
var compute_gamma_summands = function(state, predictions_certain_worlds) {
  // for each world, a list entry with obj {A > C: 0.2, A and C: 0.4, ...}
  var summands_certain_worlds = map(function(w) {
    var p_world = Math.exp(state.table.score(w))
    var prediction_certain_w = predictions_certain_worlds[w]
    var u_p_pairs = reduce(function(u, acc) {
      var p =  p_world * Math.exp(prediction_certain_w.score(u))
      return(acc.concat([[u, p]]))
    }, [], _.map(globalStore.utterances, 'utt'))

    var summand_state_w_u = Object.fromEntries(u_p_pairs)
    return(summand_state_w_u)
  }, WORLDS)

  var summands_pairs = map(function(u) {
    var summands_all_worlds_u = filter(function(s){
      return(s !== null)
    }, _.map(summands_certain_worlds, u))

    return([u, sum(summands_all_worlds_u)])
  }, _.map(globalStore.utterances, 'utt'))

  var summands = Object.fromEntries(summands_pairs)
  return(summands)
}

/*
* @return list of obj from utterance to probabilitiy
*/
var predictions_with_gamma = function(predictions, predictions_certain_worlds){
  var bn_ids = Object.keys(predictions);
  var predictions_all_bns = map(function(id) {
    var idx_state = _.map(ALL_BNS, 'bn_id').indexOf(id)
    var state = ALL_BNS[idx_state]
    var prediction_state = predictions[id]

    var predicted_probs = map(function(u){
      var p = Math.exp(prediction_state.score(u))
      return({utt: u, p: p})
    }, _.map(globalStore.utterances, 'utt'))

    var summands = compute_gamma_summands(state, predictions_certain_worlds)
    //display(sum(Object.values(summands))) // should sum up to 1
    var values_pairs = map(function(obj) {
      var p = globalStore.gamma * obj.p + (1-globalStore.gamma) * summands[obj.utt]
      return([obj.utt, p])
    }, predicted_probs)
    var values = Object.fromEntries(values_pairs)
    var prediction_with_gamma = Categorical({vs: Object.keys(values),
                                             ps: Object.values(values)});
    return([id, prediction_with_gamma])
  }, bn_ids)
  return(Object.fromEntries(predictions_all_bns))
}

var get_weighted_rsa_prediction = function(predictions, Weights) {
  //var posterior_dij = posterior_states_trial_subj[trial_id][subj]
  var obj_posterior_dij = Object.values(Object.values(Weights.params)[0])
  var states = _.map(obj_posterior_dij, 'val')
  var states_ids = _.map(states, 'bn_id')
  var p_states = _.map(obj_posterior_dij, 'prob')

  var weights = T.transpose(Vector(p_states))
  var speaker_pred = _.pick(predictions, states_ids) //{bn_id: distribution}
   // list of speaker distributions, one for each bn_id
  var distributions_speaker = Object.values(speaker_pred)
  var p_utts = Matrix(_.map(_.map(distributions_speaker, 'params'), 'ps'))
  //display(dims(p_utts))
  // display(dims(weights))
  var utts = _.uniqWith(_.map(_.map(distributions_speaker, 'params'), 'vs'), _.isEqual)
  if(utts.length != 1) error("in get_weighted_rsa_prediction unequal utterances states")
  var weighted_sums = T.dot(weights, p_utts)

  //display(dims(weighted_sums))
  var model_prediction =  Categorical({
    vs: utts[0],
    ps: T.toScalars(weighted_sums)
  })
  return(model_prediction)
}

/*
* computes P(s = <r,t> |C_i) for every context.
* returns dictionary from id to computed posterior
*/
var get_posterior_given_context = function(
  prior_conditioned_r, pars_zoib, pars_bernoulli_p_ind, contexts
) {
  var posterior_states_context_list = map(function(c_i){
    var pars_zoib_ci = filter(function(obj){
      return(obj.id == c_i)
    }, pars_zoib)
    var pars_blue = filter(function(obj){obj.p == "blue"}, pars_zoib_ci)[0]

    var P_s_given_context = Infer({
      model:function(){
        var P_r_given_ci = get_P_r_given_context(c_i)
        var r = sample(P_r_given_ci)
        var s = sample(prior_conditioned_r[r])

        var p_blue_green = Math.exp(s.table.score("AC"))
        var p_blue = p_blue_green + Math.exp(s.table.score("A-C"))
        var p_green = p_blue_green + Math.exp(s.table.score("-AC"))

        //if(s.r == "A || C"){
        if(c_i.includes("independent")) {
          var pars_green = filter(function(obj){obj.p == "green"}, pars_zoib_ci)[0]
          var ll_blue = get_zoib_likelihood(p_blue, pars_blue)
          var ll_green = get_zoib_likelihood(p_green, pars_green)

          var obs_vals = {blue: p_blue, green: p_green, AC: p_blue_green}
          var pars_bg =  Object.assign(
            filter(function(obj){obj.id == c_i}, pars_bernoulli_p_ind)[0],
            filter(function(obj){obj.p == "delta"}, pars_zoib_ci)[0]
          )
          // display(pars_bg)
          var ll_bg_delta = get_likelihood_bg(obs_vals, pars_bg)
          //display(ll_bg_delta)
          var ll = ll_blue + ll_green + ll_bg_delta
          factor(ll)

        } else { // dependent contexts
            var nbg = Math.exp(s.table.score("-AC"))
            var p_not_blue =  nbg +  Math.exp(s.table.score("-A-C"))
            var obs_vals = {
              "blue": p_blue,
              "if_bg": p_blue_green / p_blue,
              "if_nbg": nbg / p_not_blue
            }
            var ll = get_likelihood_dep_context([obs_vals], pars_zoib_ci)
            factor(ll)
        }
        return(s)
      }
    })
    return([c_i, P_s_given_context])
  }, contexts)

  return(Object.fromEntries(posterior_states_context_list))
}

var make_distr_from_slider_rating = function(slider_rating){
  return(Categorical({
    vs: ["AC", "A-C", "-AC", "-A-C"],
    ps: [slider_rating["AC"],
         slider_rating["A-C"],
         slider_rating["-AC"],
         slider_rating["-A-C"]]
  }))
}

var get_posterior_given_dij = function(P_s_given_ci, slider_rating){
  var observed_table = make_distr_from_slider_rating(slider_rating)
  var posterior = Infer({model: function(){
    var s = sample(P_s_given_ci)
    var kl = get_kulback_leibler(observed_table, s.table)
    factor(-kl)
    return(s)
  }})
  return(posterior)
}


var get_posterior_given_most_similar_state = function(slider_rating){
  var observed_table = make_distr_from_slider_rating(slider_rating)
  var best_match = reduce(function(bn, acc){
    var kl = get_kulback_leibler(observed_table, bn.table)
    return(kl < acc.kl ? {bn_id: bn.bn_id, kl: kl} : acc)
  }, {kl: Infinity, bn_id:''}, ALL_BNS)

  var posterior = Infer({model:function(){
    var s = uniformDraw(ALL_BNS)
    condition(s.bn_id == best_match.bn_id)
    return(s)
  }})
  return(posterior)
}
