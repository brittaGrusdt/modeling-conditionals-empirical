// ************ Other model parameters ************ //
globalStore.utterances = reduce(function(utt, acc) {
    var obj = {utt: utt, type: getUtteranceType(utt)}
    return(acc.concat(obj))
  }, [], data["utterances"])

var n_utts = globalStore.utterances.length
// ************ Default free model parameters ************ //
var PAR_DEFAULT = {
  alpha: 6.36, gamma: 0.368, theta: 0.9, theta_might: 0,
  // no utterance cost by default -> all utterances equally likely
  utt_cost: Object.fromEntries(
    zip(_.map(globalStore.utterances, 'utt'), repeat(n_utts, function(){0}))
  )
  //ps_utts: T.div(ones([n_utts, 1]), n_utts)
}
//display(PAR_DEFAULT.utt_cost)
// ************ Observed data to compute likelihoods ************ //
// IMPORTANT for posterior-by-trial: OBSERVATIONS must contain all utterances +
// trial combinations, even though n is 0, i.e. utterance was not observed
var OBSERVATIONS = data["observations"]
var TRIAL_IDS = _.uniq(_.map(OBSERVATIONS, 'id'))
//var PROLIFIC_IDS = _.uniq(_.map(OBSERVATIONS, 'prolific_id'))

// {trial_id: {prolific_id: observed_utt}}
// var observations_by_trial_list = map(function(trial_id){
//   var observed_trial = filter(function(obj){
//     return(obj.id == trial_id)
//   }, OBSERVATIONS)
//   var subjects = _.map(observed_trial, 'prolific_id')
//   var obj = Object.fromEntries(zip(subjects, observed_trial))
//   return([trial_id, obj])
// }, TRIAL_IDS)
//var OBSERVATIONS_BY_TRIAL_SUBJ = Object.fromEntries(observations_by_trial_list)
//display(OBSERVATIONS_BY_TRIAL_SUBJ["if1_uh"])

// ************ Prior  ************ //
// for generating tables
globalStore.n_prior_samples = data["n_forward_samples"][0]
//display(data["prior_samples"])
globalStore.state_prior = build_state_prior_from_data(data["prior_samples"])
var ALL_BNS = globalStore.state_prior.support()

// get bayes nets closest to certain world where P(x,y) is maximal, i.e. ~ 1
var BNS_CERTAIN_WORLD = map(function(world) {
  var ps_world = map(function(bn) {
    return( Math.exp(bn.table.score(world)))
  }, ALL_BNS)
  var max = reduce(function(p, acc){
    return p > acc ? p : acc
  }, 0, ps_world)
  var idx = ps_world.indexOf(max)
  var bn_world = ALL_BNS[idx]
  return({w: world, bn: bn_world, bn_id: bn_world.bn_id})
}, WORLDS)
//display(_.map(BNS_CERTAIN_WORLD, 'bn_id'))


// states applicable to utterance types
var get_states_applicable_to_utt_type = function(utt_type, thresholds) {
  var utts = filter(function(u){
    u.type == utt_type
  }, globalStore.utterances)
  var states_utt_type = filter(function(bn){
    any(function(u){meaning(u.utt, bn["table"], thresholds)}, utts)
  }, ALL_BNS)
  return(_.map(states_utt_type, 'bn_id'))
}

// ************ Functions ************ //
/** return log likelihood of observed data for 'trial_id' given model's
predictions for 'trial_id' and 'subj'
@param model_prediction <Categorical> categorical distribution over utterances
@param trial_id <str>
@param subj <str> id from a participant
**/
var get_log_likelihood_by_trial_and_subj = function(model_prediction, trial_id, subj) {
  var utt_subj_trial = filter(function(elem){
    return(elem.id == trial_id && elem.prolific_id == subj)
  }, OBSERVATIONS)[0]['utterance']
  var log_likelihood = model_prediction.score(utt_subj_trial)
  return(log_likelihood)
}

/** return log likelihood of observed data given model's predictions for respective context
@param model_prediction <Categorical> categorical distribution over utterances
@param observations_ci object id:<str>, utterance:<str>, n:<int>, ratio:<double>
**/
var get_log_likelihood_by_context = function(observations_ci, model_prediction) {

  //display(_.uniq(_.map(observations_ci, 'id'))[0])
  var utts = _.map(observations_ci, 'utterance')
  var obs_counts = _.map(observations_ci, 'n')
  //var obs_ratios = _.map(observations_ci, 'ratio')
  //display("observed utt: " + utts[2] + " n: " + obs_counts[2] + " ratio: " + obs_ratios[2])

  var predicted_ratios = map(function(u) {
    var p = Math.exp(model_prediction.score(u))
    return(p)
  }, utts)
  //display(sum(predicted_ratios))
  var likelihood = Multinomial({n: sum(obs_counts), ps: predicted_ratios})

  return(likelihood.score(obs_counts))
}

/**
* get model predictions for 'bns' using all concrete utterances
(marginalize_utts=false) or utt types (marginalize_utts=true)
@param speaker_type <str>  one of 'pragmatic' (default), 'literal', 'random'
@param by_utt_type <boolean> if true returns a distribution over utterance types,
else returns a distribution over concrete utterances
**/
var run_speaker = function(bns, speaker_type, by_utt_type){
  var distrs = map(function(bn){
    //var sp = speaker_utt_type(bn, false)
    var sp = speaker_type == 'random' ? speaker_rnd(bn, false) :
      speaker_type == 'literal' ? speaker_lit(bn, false) : speaker(bn, false);
    var prediction = by_utt_type ? marginalize(sp, 'type') : marginalize(sp, 'utt')
    return([bn.bn_id, prediction])
  }, bns)
  //var distributions = {"speaker_": distrs, "bns": bns}
  var distributions = Object.fromEntries(distrs)
  return(distributions)
}

var gaussianKernel = function(prevVal) {
  return Gaussian({mu: prevVal, sigma: 0.1});
};

var betaKernel = function(prevVal){
  var phi = 50
  var shape1 = prevVal * phi
  var shape2 = phi * (1-prevVal)
  //sanity checks, variance must be smaller than mu * (1-mu)
  //var max_var = prevVal * (1-prevVal)
  //display("mu: " + prevVal + " max_variance: " + max_var + " phi: " + phi)
  //display("shape1:" + shape1 +  " shape2: " + shape2)
  return(Beta({a: shape1, b: shape2}))
}
var exponentialKernel = function(prevVal) {
  var lambda = 1 / prevVal;
  return Exponential({a: lambda});
};

/*
* draw set of RSA-parameters to be fitted
*/
var priorSample = function(par){
  var gamma = par.includes("gamma") ?
    {gamma: sample(Uniform({a: 0, b: 1}), {driftKernel: gaussianKernel})} : {};
  // log normal distribution for alpha
  var alpha = par.includes("alpha") ? {alpha: Math.exp(sample(Gaussian({mu: 1.5, sigma: 1}), {driftKernel: gaussianKernel}))} : {};
  // var theta = par.includes("theta") ? {theta: sample(Beta({a: 2, b: 2}), {driftKernel: betaKernel})} : {};
  var theta = par.includes("theta") ? {theta: sample(Beta({a: 8, b: 2}), {driftKernel: betaKernel})} : {};
  var theta_might = par.includes("theta_might") ? {theta_might: beta({a: 2, b: 2})} : {};
  // if cost, then fix them (just n-1 cost wrt a reference category)

  var cost = par.includes("utt_cost") ?
    Object.fromEntries(map(function(u){
      var cost_u = sample(Exponential({a: 8}))
      return([u, cost_u])
    }, _.map(globalStore.utterances, 'utt'))) : {};
  var utt_cost = par.includes("utt_cost") ? {'utt_cost': cost} : {};
  //display(utt_cost)

  // var ps_utts = par.includes("ps_utts") ?
  //   {ps_utts: T.toScalars(dirichlet({alpha: ones([n_utts, 1])}))} : {};

  var all_pars = Object.assign(gamma, alpha, theta, theta_might, utt_cost)

  return all_pars
}

var setParams = function(draw){
  globalStore.alpha = draw.alpha ? draw.alpha : PAR_DEFAULT.alpha
  globalStore.gamma = draw.gamma ? draw.gamma : PAR_DEFAULT.gamma
  globalStore.thresholds = {
    theta: draw.theta ? draw.theta : PAR_DEFAULT.theta,
    theta_might: draw.theta_might ? draw.theta_might : PAR_DEFAULT.theta_might
  }
  globalStore.utt_cost = draw.utt_cost ? draw.utt_cost : PAR_DEFAULT.utt_cost;

  // informativeness of utterance types for this set of parameters
  globalStore.states_literal = get_states_applicable_to_utt_type("literal", globalStore.thresholds)
  globalStore.states_conjunction = get_states_applicable_to_utt_type("conjunction", globalStore.thresholds)
  globalStore.states_conditional = get_states_applicable_to_utt_type("conditional", globalStore.thresholds)
  globalStore.states_might = get_states_applicable_to_utt_type("might", globalStore.thresholds)
}

/** returns an object from utterances to summands of (1-gamma) part which are
computed by sum w in worlds W: P(w|s) * P_speaker(u| w)
**/
var compute_gamma_summands = function(state, predictions_certain_worlds) {
  // for each world, a list entry with obj {A > C: 0.2, A and C: 0.4, ...}
  var summands_certain_worlds = map(function(w) {
    var p_world = Math.exp(state.table.score(w))
    var prediction_certain_w = predictions_certain_worlds[w]
    var u_p_pairs = reduce(function(u, acc) {
      var p =  p_world * Math.exp(prediction_certain_w.score(u))
      return(acc.concat([[u, p]]))
    }, [], _.map(globalStore.utterances, 'utt'))

    var summand_state_w_u = Object.fromEntries(u_p_pairs)
    return(summand_state_w_u)
  }, WORLDS)

  var summands_pairs = map(function(u) {
    var summands_all_worlds_u = filter(function(s){
      return(s !== null)
    }, _.map(summands_certain_worlds, u))

    return([u, sum(summands_all_worlds_u)])
  }, _.map(globalStore.utterances, 'utt'))

  var summands = Object.fromEntries(summands_pairs)
  return(summands)
}

/*
* @return list of obj from utterance to probabilitiy
*/
var predictions_with_gamma = function(predictions, predictions_certain_worlds){
  var bn_ids = Object.keys(predictions);
  var predictions_all_bns = map(function(id) {
    var idx_state = _.map(ALL_BNS, 'bn_id').indexOf(id)
    var state = ALL_BNS[idx_state]
    var prediction_state = predictions[id]

    var predicted_probs = map(function(u){
      var p = Math.exp(prediction_state.score(u))
      return({utt: u, p: p})
    }, _.map(globalStore.utterances, 'utt'))

    var summands = compute_gamma_summands(state, predictions_certain_worlds)
    //display(sum(Object.values(summands))) // should sum up to 1
    var values_pairs = map(function(obj) {
      var p = globalStore.gamma * obj.p + (1-globalStore.gamma) * summands[obj.utt]
      return([obj.utt, p])
    }, predicted_probs)
    var values = Object.fromEntries(values_pairs)
    var prediction_with_gamma = Categorical({vs: Object.keys(values),
                                             ps: Object.values(values)});
    return([id, prediction_with_gamma])
  }, bn_ids)
  return(Object.fromEntries(predictions_all_bns))
}

/*
* @param predictions: object from bn_id to speaker predictions <str: Distribution P_S(u|s)>
* @param states_ids: list of ids <str>
* @param p_states: list of probabilities of 'states_ids'
*/
var get_weighted_rsa_prediction = function(predictions, states_ids, p_states) {
  var weights = T.transpose(Vector(p_states))
  var speaker_pred = _.pick(predictions, states_ids)
   // list of speaker distributions, one for each bn_id
  var distributions_speaker = Object.values(speaker_pred)
  var p_utts = Matrix(_.map(_.map(distributions_speaker, 'params'), 'ps'))
  //display(dims(p_utts))
  // display(dims(weights))
  var utts = _.uniqWith(_.map(_.map(distributions_speaker, 'params'), 'vs'), _.isEqual)
  if(utts.length != 1) error("in get_weighted_rsa_prediction unequal utterances states")
  var weighted_sums = T.dot(weights, p_utts)

  //display(dims(weighted_sums))
  var model_prediction =  Categorical({
    vs: utts[0],
    ps: T.toScalars(weighted_sums)
  })
  return(model_prediction)
}

/*
* computes P(s = <r,t> |C_i) for every context.
* returns dictionary from context id to computed posterior
*/
var get_posterior_given_context = function(
  prior_conditioned_r, pars_zoib, par_gaussian, contexts, prior_rel
) {
  var posterior_states_context_list = map(function(c_i){
    var pars_zoib_ci = filter(function(obj){
      return(obj.id == c_i)
    }, pars_zoib)
    var pars_blue = filter(function(obj){obj.p == "blue"}, pars_zoib_ci)[0]

    var P_s_given_context = Infer({
      model:function(){
        var P_r_given_ci = get_P_r_given_context(c_i, prior_rel)

        var r = sample(P_r_given_ci)
        //display("r: " + r)
        var s = sample(prior_conditioned_r[r])
        //display("s: " + s)

        var p_blue_green = Math.exp(s.table.score("AC"))
        var p_blue = p_blue_green + Math.exp(s.table.score("A-C"))
        var p_green = p_blue_green + Math.exp(s.table.score("-AC"))

        //if(s.r == "A || C"){
        if(c_i.includes("independent")) {
          var pars_green = filter(function(obj){obj.p == "green"}, pars_zoib_ci)[0]
          var ll_blue = get_zoib_likelihood(p_blue, pars_blue)
          var ll_green = get_zoib_likelihood(p_green, pars_green)

          var obs_vals = {blue: p_blue, green: p_green, AC: p_blue_green}
          var pars_bg =  Object.assign(
            filter(function(obj){obj.id == c_i}, par_gaussian)[0],
            filter(function(obj){obj.p == "delta"}, pars_zoib_ci)[0]
          )
          //display(pars_bg)
          var ll_bg_delta = get_likelihood_bg(obs_vals, pars_bg)
          //display(ll_bg_delta)
          var ll = ll_blue + ll_green + ll_bg_delta
          factor(ll)

        } else { // dependent contexts
            var nbg = Math.exp(s.table.score("-AC"))
            var p_not_blue =  nbg +  Math.exp(s.table.score("-A-C"))
            var obs_vals = {
              "blue": p_blue,
              "if_bg": p_blue_green / p_blue,
              "if_nbg": nbg / p_not_blue
            }
            var ll = get_likelihood_dep_context([obs_vals], pars_zoib_ci)
            factor(ll)
        }
        return(s)
      }
    })
    return([c_i, P_s_given_context])
  }, contexts)

  return(Object.fromEntries(posterior_states_context_list))
}

var make_distr_from_slider_rating = function(slider_rating){
  return(Categorical({
    vs: ["AC", "A-C", "-AC", "-A-C"],
    ps: [slider_rating["AC"],
         slider_rating["A-C"],
         slider_rating["-AC"],
         slider_rating["-A-C"]]
  }))
}


/**
* Default context prior over world states
* @param samples states provided via R, sampled beforehand
* (e.g., by run-state-prior.wppl)
*
@return object from context to distributions
*/
var build_posterior_states_given_context_from_data = function(samples, contexts){

  var distr_list = map(function(context){
    var samples_ci = filter(function(sample){
      return(sample.id == context)
    }, samples)
    var probs_s_ci = _.map(samples_ci, 'probs')
    var states_ci = map(function(obj){
      var vs = obj["table.support"]
      var ps = obj["table.probs"]
      var Table = Categorical({vs, ps})
      var state = {
        r: obj.r,
        probability: obj.probability,
        table: Table,
        bn_id: obj.bn_id,
        cn: obj.cn
      }
      return(state)
    }, samples_ci)
    var bn_ids_ci = _.map(states_ci, 'bn_id')
    //display(bn_ids_ci)
    var P_s_ci = Categorical({vs: states_ci, ps: probs_s_ci})

    return([context, P_s_ci])
  }, contexts)

  return(Object.fromEntries(distr_list))
}


var run_rsa_model = function(data){
  var params = {
    alpha: data.alpha,
    gamma: data.gamma,
    theta: data.theta,
    utt_cost: Object.fromEntries(
     zip(_.map(data.utt_cost, 'utterance'), _.map(data.utt_cost, 'cost'))
    )
  }
  //display(params)
  setParams(params)
  // just for debugging
  // var invalid_utts_states = check_states_utts(
  //   ALL_BNS,
  //   _.map(globalStore.utterances, 'utt'),
  //   globalStore.thresholds,
  //   params,
  //   true
  // )
  var uc_data = data["observed_utts_ratios"]
  var prior_rel = data["prior_relations"]
  var par_zoibs = data["likelihoods_zoib"]
  var par_gaussian = data["likelihoods_gaussian"]
  var contexts = _.uniq(_.map(par_zoibs, 'id')).concat(_.uniq(_.map(par_gaussian, 'id')))

  var speaker_type = data["speaker_type"]

  if(data["verbose"][0]) {
    display("free parameters:")
    display("alpha: " + globalStore.alpha)
    display("gamma: " + globalStore.gamma)
    display("theta: " + globalStore.thresholds.theta)
    display("theta_might: " + globalStore.thresholds.theta_might)
    display('informativeness utterance types:')
    display("# conjunctions: " + globalStore.states_conjunction.length)
    display("# literals: " + globalStore.states_literal.length)
    display("# conditionals: " + globalStore.states_conditional.length)
    display("# mights: " + globalStore.states_might.length)
    display("contexts: " + contexts)
  }

  // get Bayes nets used for predictions for each trial + participant
  // we sample a certain number of Bayes nets per relation
  // (depending on P(r|data), computed beforehand in R)
  // // ONE DISTRIBUTION FOR EACH CN: PRIOR CONDITIONED ON CN
  var priors_conditioned = map(function(r) {
    var prior_conditioned_r = Infer({model: function() {
        var s = sample(globalStore.state_prior)
        condition(s.r == r)
        return(s)
      }, method: 'enumerate'})
    return([r, prior_conditioned_r])
  }, RELATIONS)
  var prior_conditioned_r = Object.fromEntries(priors_conditioned)

  var posterior_states_contexts = get_posterior_given_context(
    prior_conditioned_r, par_zoibs, par_gaussian, contexts, prior_rel
  )

  // RUN MODEL
  var rsa_speaker_predictions = run_speaker(ALL_BNS, speaker_type, false)
  //display(rsa_speaker_predictions)
  //var x = rsa_speaker_predictions['-A implies C_unc-high-low_0.006224644725537807_0.550477986004518_0.43128676552264295_0.012010603747301285']
  // predictions 4 (close to) certain worlds
  var predictions_certain_worlds_list = map(function(bn){
    return([bn.w, rsa_speaker_predictions[bn.bn_id]])
    //return({w: bn.w, prediction: rsa_speaker_predictions[bn.bn_id]})
  }, BNS_CERTAIN_WORLD)
  var predictions_certain_worlds = Object.fromEntries(predictions_certain_worlds_list)
  var speaker_predictions = predictions_with_gamma(rsa_speaker_predictions,
                                                   predictions_certain_worlds)

  // get predictions + compute log likelihood of utterance choices given model predictions
  var result = map(function(c_i) {
    // some single trials were excluded
    var observations_ci = filter(function(obj){
      return(obj.id == c_i)
    }, uc_data)

    var Weights = posterior_states_contexts[c_i]
    // note: Object.values(Weights.params) returns a list with an object
    // mapping from support-values to an object
    // (Weights is a distribution built with Infer, not a specific built-in Distribution!
    // this is important because otherweise Weights.params returns a different structure!)
    // {val: `the respective support value', prob: `its probability`} -> using twice Object.values()!
    var state_prob_pairs = Object.values(Object.values(Weights.params)[0])
    var states = _.map(state_prob_pairs, 'val')
    var states_ids = _.map(states, 'bn_id')
    var states_probs = _.map(state_prob_pairs, 'prob')

    var model_prediction = get_weighted_rsa_prediction(
      speaker_predictions, states_ids, states_probs
    )
    // display(model_prediction)

    var result_by_ci = {
      ll_ci: get_log_likelihood_by_context(observations_ci, model_prediction),
      p_hat: model_prediction.params.ps,
      utterance: model_prediction.params.vs,
      id: c_i
    }

    return([c_i, result_by_ci])
  }, contexts)

  //var summed_log_likelihood = sum(_.map(log_likelihood_by_trial, 'll'))
  return(Object.fromEntries(result))
}
