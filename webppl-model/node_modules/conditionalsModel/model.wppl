// ************ Other model parameters ************ //
globalStore.utterances = reduce(function(utt, acc) {
    var obj = {utt: utt, type: getUtteranceType(utt)}
    return(acc.concat(obj))
  }, [], data["utterances"])

// ************ Default free model parameters ************ //
var PAR_DEFAULT = {alpha: 6.36, gamma: 0.368, theta: 0.9, theta_might: 0,
  cost_conditional: 0, cost_might: 0, cost_conjunction: 0, cost_literal: 0,
  // default: all utterances equally likely
  p_utts: T.div(ones([globalStore.utterances.length, 1]), globalStore.utterances.length)
}

// ************ Observed data to compute likelihoods ************ //
// IMPORTANT for posterior-by-trial: OBSERVATIONS must contain all utterances +
// trial combinations, even though n is 0, i.e. utterance was not observed
var OBSERVATIONS = data["observations"]
var TRIAL_IDS = _.uniq(_.map(OBSERVATIONS, 'id'))
var PROLIFIC_IDS = _.uniq(_.map(OBSERVATIONS, 'prolific_id'))

// {trial_id: {prolific_id: observed_utt}}
var observations_by_trial_list = map(function(trial_id){
  var observed_trial = filter(function(obj){
    return(obj.id == trial_id)
  }, OBSERVATIONS)
  var subjects = _.map(observed_trial, 'prolific_id')
  var obj = Object.fromEntries(zip(subjects, observed_trial))
  return([trial_id, obj])
}, TRIAL_IDS)
var OBSERVATIONS_BY_TRIAL_SUBJ = Object.fromEntries(observations_by_trial_list)

// var PROLIFIC_IDS = _.uniq(_.map(OBSERVATIONS, 'prolific_id'))
// ************ Prior  ************ //
// for generating tables
globalStore.n_prior_samples = data["n_forward_samples"][0]
//display(data["prior_samples"])
globalStore.state_prior = build_state_prior_from_data(data["prior_samples"])
var ALL_BNS = globalStore.state_prior.support()

// get bayes nets closest to certain world where P(x,y) is maximal, i.e. ~ 1
var BNS_CERTAIN_WORLD = map(function(world) {
  var ps_world = map(function(bn) {
    return( Math.exp(bn.table.score(world)))
  }, ALL_BNS)
  var max = reduce(function(p, acc){
    return p > acc ? p : acc
  }, 0, ps_world)
  var idx = ps_world.indexOf(max)
  var bn_world = ALL_BNS[idx]
  return({w: world, bn: bn_world, bn_id: bn_world.bn_id})
}, WORLDS)


// states applicable to utterance types
var get_states_applicable_to_utt_type = function(utt_type, thresholds) {
  var utts = filter(function(u){
    u.type == utt_type
  }, globalStore.utterances)
  var states_utt_type = filter(function(bn){
    any(function(u){meaning(u.utt, bn["table"], thresholds)}, utts)
  }, ALL_BNS)
  return(_.map(states_utt_type, 'bn_id'))
}

// ************ Functions ************ //
/** return log likelihood of observed data for 'trial_id' given model's
predictions for 'trial_id' and 'subj'
@arg model_prediction: categorical distribution over utterances
@arg trial_id: str
@arg subj: str, id from a participant
**/
var get_log_likelihood_by_trial_and_subj = function(model_prediction, trial_id, subj) {
  var utt_subj_trial = filter(function(elem){
    return(elem.id == trial_id && elem.prolific_id == subj)
  }, OBSERVATIONS)[0]['utterance']
  var log_likelihood = model_prediction.score(utt_subj_trial)
  return(log_likelihood)
}

/** return log likelihood of observed data for 'trial_id' given model's
predictions for 'trial_id'
@arg model_prediction: categorical distribution over utterances
@arg trial_id: str
**/
var get_log_likelihood_by_trial = function(model_prediction, trial_id) {
  var observations_trial = filter(function(elem){
    return(elem.id == trial_id)
  }, OBSERVATIONS)

  var utts = _.map(observations_trial, 'utterance')
  var observed_counts = map(function(n){
    return(Number.parseInt(n))
  }, _.map(observations_trial, 'n'))

  var observed_ratios = map(function(n){
    return(n / sum(observed_counts))
  }, observed_counts)

  var predicted_ratios = map(function(u) {
    var p = Math.exp(model_prediction.score(u))
    return(p)
  }, utts)

  var likelihood = Multinomial({n: sum(observed_counts), ps: predicted_ratios})
  return(likelihood.score(observed_counts))
}

//get model predictions for 'bns' using all concrete utterances or utt types
var run_speaker = function(bns, by_utt_type){
  var distrs = map(function(bn){
    var sp = speaker_utt_type(bn, false)
    var prediction = by_utt_type ? marginalize(sp, 'type') : marginalize(sp, 'utt')
    return([bn.bn_id, prediction])
  }, bns)
  //var distributions = {"speaker_": distrs, "bns": bns}
  var distributions = Object.fromEntries(distrs)
  return(distributions)
}

// draw set of parameters to be fitted
var priorSample = function(par){
  var gamma = par.includes("gamma") ? {gamma: beta({a: 1, b: 1})} : {};
  // var alpha = par.includes("alpha") ? {alpha: uniform({a: 0, b: 10})} : {};
  var alpha = par.includes("alpha") ? {alpha: sample(lognormal(1, 1))} : {};
  var theta = par.includes("theta") ? {theta: beta({a: 2, b: 2})} : {};
  var theta_might = par.includes("theta_might") ? {theta_might: beta({a: 2, b: 2})} : {};
  // if cost, then fix them (just n-1 cost wrt a reference category)
  // var cost_conditional = par.includes("cost_conditional") ? {cost_conditional: uniform({a: -0.5, b: 0.5})} : {};
  // var cost_conjunction = par.includes("cost_conjunction") ? {cost_conjunction: uniform({a: -0.5, b: 0.5})} : {};
  // var cost_literal = par.includes("cost_literal") ? {cost_literal: uniform({a: -0.5, b: 0.5})} : {};
  // var cost_might = par.includes("cost_might") ? {cost_might: uniform({a: -0.5, b: 0.5})} : {};
  var p_utts = par.includes("p_utts") ? {ps_utts: T.toScalars(dirichlet({alpha: ones([globalStore.utterances.length, 1])}))} : {};
  var all_pars = Object.assign(gamma, alpha, theta, theta_might, p_utts);
    // cost_conditional, cost_conjunction, cost_literal, cost_might);

  return all_pars
}

var setParams = function(draw){
  globalStore.alpha = draw.alpha ? draw.alpha : PAR_DEFAULT.alpha
  globalStore.gamma = draw.gamma ? draw.gamma : PAR_DEFAULT.gamma
  globalStore.thresholds = {
    theta: draw.theta ? draw.theta : PAR_DEFAULT.theta,
    theta_might: draw.theta_might ? draw.theta_might : PAR_DEFAULT.theta_might
  }
  globalStore.ps_utts = draw.p_utts ? draw.p_utts : PAR_DEFAULT.p_utts;
  // globalStore.cost = {
  //   conditional: draw.cost_conditional ? draw.cost_conditional : PAR_DEFAULT.cost_conditional,
  //   conjunction: draw.cost_conjunction ? draw.cost_conjunction : PAR_DEFAULT.cost_conjunction,
  //   literal: draw.cost_literal ? draw.cost_literal : PAR_DEFAULT.cost_literal,
  //   might: draw.cost_might ? draw.cost_might : PAR_DEFAULT.cost_might
  // }

  // informativeness of utterance types for this set of parameters
  globalStore.states_literal = get_states_applicable_to_utt_type("literal", globalStore.thresholds)
  globalStore.states_conjunction = get_states_applicable_to_utt_type("conjunction", globalStore.thresholds)
  globalStore.states_conditional = get_states_applicable_to_utt_type("conditional", globalStore.thresholds)
  globalStore.states_might = get_states_applicable_to_utt_type("might", globalStore.thresholds)
}

/** returns an object from utterances to summands of (1-gamma) part which are
computed by sum w in worlds W: P(w|s) * P_speaker(u| w)
**/
var compute_gamma_summands = function(state, predictions_certain_worlds) {
  // for each world, a list entry with obj {A > C: 0.2, A and C: 0.4, ...}
  var summands_certain_worlds = map(function(w) {
    var p_world = Math.exp(state.table.score(w))
    var prediction_certain_w = predictions_certain_worlds[w]
    var u_p_pairs = reduce(function(u, acc) {
      var p =  p_world * Math.exp(prediction_certain_w.score(u))
      return(acc.concat([[u, p]]))
    }, [], _.map(globalStore.utterances, 'utt'))

    var summand_state_w_u = Object.fromEntries(u_p_pairs)
    return(summand_state_w_u)
  }, WORLDS)

  var summands_pairs = map(function(u) {
    var summands_all_worlds_u = filter(function(s){
      return(s !== null)
    }, _.map(summands_certain_worlds, u))

    return([u, sum(summands_all_worlds_u)])
  }, _.map(globalStore.utterances, 'utt'))

  var summands = Object.fromEntries(summands_pairs)
  return(summands)
}

/*
* @return list of obj from utterance to probabilitiy
*/
var predictions_with_gamma = function(predictions, predictions_certain_worlds){
  var bn_ids = Object.keys(predictions);
  var predictions_all_bns = map(function(id) {
    var idx_state = _.map(ALL_BNS, 'bn_id').indexOf(id)
    var state = ALL_BNS[idx_state]
    var prediction_state = predictions[id]

    var predicted_probs = map(function(u){
      var p = Math.exp(prediction_state.score(u))
      return({utt: u, p: p})
    }, _.map(globalStore.utterances, 'utt'))

    var summands = compute_gamma_summands(state, predictions_certain_worlds)
    //display(sum(Object.values(summands))) // should sum up to 1
    var values_pairs = map(function(obj) {
      var p = globalStore.gamma * obj.p + (1-globalStore.gamma) * summands[obj.utt]
      return([obj.utt, p])
    }, predicted_probs)
    var values = Object.fromEntries(values_pairs)
    var prediction_with_gamma = Categorical({vs: Object.keys(values),
                                             ps: Object.values(values)});
    return([id, prediction_with_gamma])
  }, bn_ids)
  return(Object.fromEntries(predictions_all_bns))
}

var get_prediction_per_trial_and_subj = function(predictions, trial_id, subj,
                                                 posterior_states_trial_subj) {
  var posterior_dij = posterior_states_trial_subj[trial_id][subj]
  var obj_posterior_dij = Object.values(Object.values(posterior_dij.params)[0])
  var states = _.map(obj_posterior_dij, 'val')
  var states_ids = _.map(states, 'bn_id')
  var p_states = _.map(obj_posterior_dij, 'prob')

  var weights = T.transpose(Vector(p_states))
  var speaker_pred = _.pick(predictions, states_ids) //{bn_id: distribution}
   // list of speaker distributions, one for each bn_id
  var distributions_speaker = Object.values(speaker_pred)
  var p_utts = Matrix(_.map(_.map(distributions_speaker, 'params'), 'ps'))
  //display(dims(p_utts))
  // display(dims(weights))
  var utts = _.uniqWith(_.map(_.map(distributions_speaker, 'params'), 'vs'), _.isEqual)
  if(utts.length != 1) error("in get_prediction_per_trial_and_subj unequal utterances states")
  var weighted_sums = T.dot(weights, p_utts)

  //display(dims(weighted_sums))
  var model_prediction =  Categorical({
    vs: utts[0],
    ps: T.toScalars(weighted_sums)
  })
  return(model_prediction)
}
